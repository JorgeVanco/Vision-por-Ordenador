{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proyecto Final Visión por ordenador\n",
    "\n",
    "#### Integrantes: María González Gómez y Jorge Vanco Sampedro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Cargamos las librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import *\n",
    "from bow import BoW\n",
    "from dataset import Dataset\n",
    "from image_classifier import ImageClassifier\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Carga del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\meryg\\.cache\\kagglehub\\datasets\\ahemateja19bec1025\\traffic-sign-dataset-classification\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"ahemateja19bec1025/traffic-sign-dataset-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## código usado por si fuera necesario renombrar carpetas \n",
    "\n",
    "# import shutil\n",
    "# num_to_label = pd.read_csv(\"../data/labels.csv\", index_col=\"ClassId\").to_dict()[\"Name\"]\n",
    "# num_to_label = {str(k): v for k,v in num_to_label.items()}\n",
    "# num_to_label = {str(k): v.replace('/', '') for k, v in num_to_label.items()}\n",
    "# print(num_to_label)\n",
    "\n",
    "# train_data_dir = \"../data/traffic_Data/DATA/\"\n",
    "# for dir in os.listdir(train_data_dir):\n",
    "#     if dir in num_to_label.keys():\n",
    "#         dir_path = os.path.join(train_data_dir, dir)\n",
    "#         new_dir_path = os.path.join(train_data_dir, num_to_label[dir])\n",
    "#         print(f\"Renaming {dir_path} to {new_dir_path}\")\n",
    "#         if os.path.exists(dir_path):\n",
    "#             if os.path.exists(new_dir_path):\n",
    "#                 shutil.move(dir_path, new_dir_path)\n",
    "#             else:\n",
    "#                 os.renames(dir_path, new_dir_path)\n",
    "#         else:\n",
    "#             print(f\"Path {dir_path} does not exist\")\n",
    "\n",
    "# test_data_dir = \"../data/traffic_Data/TEST/\"\n",
    "\n",
    "# for image in os.listdir(test_data_dir):\n",
    "#     num = image[:3]\n",
    "#     try:\n",
    "#         num = str(int(num))\n",
    "#         label = num_to_label[num]\n",
    "#         dir_path = os.path.join(test_data_dir, label)\n",
    "#         if not os.path.exists(dir_path):\n",
    "#             os.makedirs(dir_path)\n",
    "\n",
    "#         image_path = os.path.join(test_data_dir, image)\n",
    "#         shutil.move(image_path, dir_path)\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/traffic_Data/DATA\\Bicycles crossing\\030_0001.png\n",
      "../data/traffic_Data/TEST\\Bicycles crossing\\030_0001_j.png\n"
     ]
    }
   ],
   "source": [
    "# Cargar conjuntos de datos\n",
    "training_set = Dataset.load(\"../data/traffic_Data/DATA\", \"*png\")\n",
    "validation_set = Dataset.load(\"../data/traffic_Data/TEST\", \"*png\")\n",
    "\n",
    "print(training_set[0])\n",
    "print(validation_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Creación del extractor de carácterísticas SIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el extractor de características SIFT\n",
    "feature_extractor = cv2.SIFT_create()\n",
    "\n",
    "# Extraer descriptores\n",
    "print(\"\\nComputing SIFT descriptors...\")\n",
    "time.sleep(0.1)  # Previene problemas de concurrencia entre tqdm y print\n",
    "\n",
    "descriptors = []  # Lista para almacenar descriptores\n",
    "for path in tqdm(training_set, unit=\"image\", file=sys.stdout):\n",
    "    # Cargar la imagen en escala de grises\n",
    "    image = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    try:\n",
    "        # Detectar y describir características SIFT\n",
    "        keypoints, descriptor = feature_extractor.detectAndCompute(image, None)\n",
    "        if descriptor is not None:\n",
    "            descriptors.append(descriptor)\n",
    "        else:\n",
    "            print(f\"Advertencia: No se encontraron descriptores en {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {path}: {e}\")\n",
    "print(f\"Número de descriptores extraídos: {len(descriptors)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Creación del vocabulario con K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "vocabulary_size = 200\n",
    "iterations = 20\n",
    "termination_criteria = (cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_EPS, iterations, 1e-6)\n",
    "\n",
    "# Initialize BOWKMeansTrainer\n",
    "words = cv2.BOWKMeansTrainer(vocabulary_size, termination_criteria)\n",
    "\n",
    "# Add descriptors to the trainer\n",
    "for desc in tqdm(descriptors, desc=\"Adding descriptors\"):\n",
    "    words.add(desc)\n",
    "\n",
    "time.sleep(0.1)  # Prevent tqdm printing issues\n",
    "print(\"\\nClustering descriptors into\", vocabulary_size, \"words using K-means...\")\n",
    "\n",
    "# Perform k-means clustering to build the vocabulary\n",
    "vocabulary = words.cluster()\n",
    "print(f\"Vocab: {vocabulary}\")\n",
    "filename=  \"vocabulary.pickle\"\n",
    "# TODO: Open the file from above in the write and binay mode\n",
    "with open(filename,\"wb\") as f:\n",
    "    pickle.dump([\"SIFT\", vocabulary], f, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "try:\n",
    "    with open(\"vocabulary.pickle\", \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "        print(f\"Data from vocab.pickle: {data}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Inferencia en Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load vocabulary into BoW\n",
    "bow = BoW()\n",
    "bow.load_vocabulary(filename.strip(\".pickle\"))\n",
    "\n",
    "# Train the image classifier\n",
    "image_classifier = ImageClassifier(bow)\n",
    "print(f\"Image classifier: {image_classifier}\")\n",
    "image_classifier.train(training_set, iterations)\n",
    "\n",
    "# Save the trained classifier\n",
    "classifier = \"classifier\"\n",
    "image_classifier.save(classifier)\n",
    "\n",
    "print(\"Vocabulary and classifier successfully built and saved.\")\n",
    "\n",
    "\n",
    "print(\"Empezando el train: \")\n",
    "\n",
    "bow = BoW()\n",
    "# TODO: Especify the args for the loading method\n",
    "bow.load_vocabulary(filename.strip(\".pickle\"))\n",
    "\n",
    "image_classifier = ImageClassifier(bow)\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.load(classifier)\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.predict(training_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Inferencia en Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Empezando el test: \")\n",
    "bow = BoW()\n",
    "# TODO: Especify the args for the loading method\n",
    "bow.load_vocabulary(filename.strip(\".pickle\"))\n",
    "\n",
    "image_classifier = ImageClassifier(bow)\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.load(classifier)\n",
    "# TODO: Especify the args for the loading method\n",
    "image_classifier.predict(validation_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Tracking de señales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9: Clasificación de señales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga tu modelo SVM entrenado\n",
    "import joblib\n",
    "svm_model = joblib.load(\"classifier_model.xml.pkl\")\n",
    "\n",
    "# Preprocesa el ROI y extrae características (ejemplo con HOG)\n",
    "hog = cv2.HOGDescriptor()\n",
    "features = hog.compute(cv2.resize(roi, (64, 64)))\n",
    "\n",
    "# Clasifica la señal\n",
    "prediction = svm_model.predict(features.reshape(1, -1))\n",
    "print(f\"Señal detectada: {prediction[0]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
